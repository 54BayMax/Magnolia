{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forked Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Standard python libraries\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pylab as plt\n",
    "import functools\n",
    "%matplotlib inline\n",
    "\n",
    "## Magnolia data iteration\n",
    "sys.path.append('../../')\n",
    "from src.features.mixer import FeatureMixer\n",
    "from src.features.wav_iterator import batcher\n",
    "from src.features.supervised_iterator import SupervisedIterator, SupervisedMixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0-rc2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batchsize = 64\n",
    "datashape = (16, 257)\n",
    "embedding_size = 600\n",
    "libridev='/local_data/teams/magnolia/libri-dev.h5'\n",
    "libritrain='/local_data/teams/magnolia/processed_train-clean-100.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a supervised mixer and batcher\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised feature mixer with 3 libridev sources timed at  0.4762789999999999 sec\n"
     ]
    }
   ],
   "source": [
    "mixer = SupervisedMixer([libritrain,libritrain,libritrain], shape=datashape, \n",
    "                     mix_method='add', diffseed=True, return_key=True)\n",
    "# Check the time\n",
    "tbeg = time.clock()\n",
    "X, Y, I = mixer.get_batch(batchsize)\n",
    "tend = time.clock()\n",
    "print('Supervised feature mixer with 3 libridev sources timed at ', (tend-tbeg), 'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEURAL NETWORK\n",
    "\n",
    "The lost function takes in as input the variable `Vlast` for last layer ($V_{last}$, where a vector in $V_{last}$ is $v_{l}$). (That's the first couplet lines, where one just makes a tensorflow variable `Vlasttf`.)\n",
    "\n",
    "The actual cost function is the *word2vec* objective function, where samples are positively and negatively sampled and then mixed. Let $A$ be a matrix of \"attractors\", so to speak. (We'll not use that terminology later on.) Then a positively sampled vector $a_p$ and a few negatively sampled ones $a_{n_1}$ and $a_{n_2}$ are all columns in $A$. The loss over a batch $B$ is denoted `tfbatchlo`, and is specified as:\n",
    "\n",
    "$$ \\mathcal{L}(v_{last}) = \\log \\sigma ( v_l^T a_p) + \\sum_j \\log \\sigma( -1 \\cdot v_l^T a_{n_j} )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forked neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scope(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "    name = function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self,attribute):\n",
    "            with tf.device(\"/gpu:0\"):\n",
    "                with tf.variable_scope(name):\n",
    "                    setattr(self,attribute,function(self))\n",
    "        return getattr(self,attribute)\n",
    "    \n",
    "    return decorator\n",
    "\n",
    "class ClusterModel:\n",
    "    def __init__(self, X, Y, F, I, layer_size, embedding_size, num_labels):\n",
    "        \n",
    "        self.Vclass = tf.Variable(tf.random_normal( [embedding_size, num_labels, F] ), dtype=tf.float32)\n",
    "        \n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "        self.F = F\n",
    "        self.I = I\n",
    "        \n",
    "        self.layer_size = layer_size\n",
    "        self.embedding_size = embedding_size\n",
    "                \n",
    "        self.network\n",
    "        self.cost\n",
    "        self.optimizer\n",
    "        \n",
    "    \n",
    "    def weight_variable(self,shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=tf.sqrt(2.0/shape[0]))\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def conv1d(self,x, W):\n",
    "        return tf.nn.conv1d(x, W, stride=1, padding='SAME')\n",
    "    \n",
    "    def conv1d_layer(self,in_layer,shape):\n",
    "        weights = self.weight_variable(shape)\n",
    "        biases = self.weight_variable([shape[-1]])\n",
    "        \n",
    "        return self.conv1d(in_layer,weights) + biases\n",
    "    \n",
    "    def BLSTM(self, X, size, scope):\n",
    "        forward_input = X\n",
    "        backward_input = tf.reverse(X, [1])\n",
    "        \n",
    "        with tf.variable_scope('forward_' + scope):\n",
    "            forward_lstm = tf.contrib.rnn.BasicLSTMCell(size//2)\n",
    "            forward_out, f_state = tf.nn.dynamic_rnn(forward_lstm, forward_input, dtype=tf.float32)\n",
    "        \n",
    "        with tf.variable_scope('backward_' + scope):\n",
    "            backward_lstm = tf.contrib.rnn.BasicLSTMCell(size//2)\n",
    "            backward_out, b_state = tf.nn.dynamic_rnn(backward_lstm, backward_input, dtype=tf.float32)\n",
    "        \n",
    "        return tf.concat([forward_out[:,:,:], backward_out[:,::-1,:]], 2)\n",
    "    \n",
    "    @scope\n",
    "    def network(self):\n",
    "        shape = tf.shape(self.X)\n",
    "        \n",
    "        BLSTM_1 = self.BLSTM(self.X, self.layer_size, 'one')\n",
    "        BLSTM_2 = self.BLSTM(BLSTM_1, self.layer_size, 'two')\n",
    "        \n",
    "        feedforward = self.conv1d_layer(BLSTM_2,[1,self.layer_size,self.embedding_size*self.F])\n",
    "        \n",
    "        embedding = tf.reshape(feedforward,[shape[0],shape[1],self.F,self.embedding_size]) \n",
    "        embedding = tf.nn.l2_normalize(embedding,3)\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    @scope\n",
    "    def cost(self):        \n",
    "        \n",
    "        Xshape=tf.shape(self.X)\n",
    "        Yshape=tf.shape(self.Y)\n",
    "        \n",
    "        # things that are necessary for the cost function\n",
    "        Vin = self.network\n",
    "        I = tf.expand_dims( self.I, axis=2 )\n",
    "        Y = self.Y\n",
    "        Vclass = self.Vclass\n",
    "        \n",
    "        print(tf.shape(Vin))\n",
    "        \n",
    "        # gather the appropriate vectors\n",
    "        Vout = tf.gather_nd( tf.transpose(Vclass, perm=[1,2,0]), I )\n",
    "        \n",
    "        # Broadcasted Vi and Vo\n",
    "        Vinbroad = tf.reshape( Vin, [Yshape[0], 1, Yshape[2], Yshape[3], self.embedding_size])\n",
    "        Voutbroad= tf.reshape( Vout, [Yshape[0], Yshape[1], 1, Yshape[3], self.embedding_size] )\n",
    "                \n",
    "        # Correlate all the vectors:\n",
    "        lossfxn = - tf.log( tf.nn.sigmoid( - Y * tf.reduce_sum(Vinbroad * Voutbroad, 4) ) )\n",
    "        \n",
    "        # Sum correlations over positive and negative correlations\n",
    "        lossfxn = tf.reduce_sum( lossfxn, 1 )\n",
    "        \n",
    "        # Average over all the batches\n",
    "        lossfxn = tf.reduce_mean( lossfxn, 0)\n",
    "        \n",
    "        # To do: put weight by pre-emphasis or gradient confidence\n",
    "        lossfxn = tf.reduce_mean( lossfxn )\n",
    "        \n",
    "        return lossfxn\n",
    "\n",
    "    @scope\n",
    "    def optimizer(self):\n",
    "        opt = tf.train.AdamOptimizer()\n",
    "        cost = self.cost\n",
    "        return opt.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"cost/Shape_2:0\", shape=(4,), dtype=int32, device=/device:GPU:0)\n",
      "Initialized\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "F = 257\n",
    "layer_size=50\n",
    "embedding_size=300\n",
    "X = tf.placeholder(\"float\", [None,None,F])\n",
    "Y = tf.placeholder(\"float\", [None, None,None,F])\n",
    "I = tf.placeholder(dtype=tf.int32)\n",
    "\n",
    "num_labels=251\n",
    "\n",
    "model = ClusterModel(X, Y, F, I, layer_size, embedding_size, num_labels)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "iterations = []\n",
    "costs = []\n",
    "\n",
    "print(\"Initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function = 1.90061"
     ]
    }
   ],
   "source": [
    "costs = []\n",
    "for iteration in range(1000):\n",
    "    Xdata, Ydata, Idata = mixer.get_batch(64, out_TF=None)\n",
    "    Ydata = Ydata.reshape( [Ydata.shape[0], Ydata.shape[1], 16, 257] )\n",
    "\n",
    "    optloss, cost = sess.run([model.optimizer, model.cost], feed_dict={X: abs(Xdata), Y:Ydata, I:Idata})\n",
    "    costs += [cost]\n",
    "    sys.stdout.write('\\rCost function = '+str(cost))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3tf1",
   "language": "python",
   "name": "py3tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse NMF for Separation\n",
    "\n",
    "One approach to using NMF for source separation is to learn sets of $k$ basis vectors $W$ and $H$ for each of the speakers in a dataset. To separate a mixture where the speakers are known, we concatenate the dictionaries $W$ associated with them, learn new loadings $H$, and take the product of the speaker-specific $W$ with the associated components of $H$ to yield the reconstruction.\n",
    "\n",
    "Training, for speaker $i$:\n",
    "$$X_i = W_i H_i$$\n",
    "$$W_i, H_i = \\text{NMF}(X_i)$$\n",
    "\n",
    "Evaluation, on mixture $X_{ij}$ of speech from speaker $i$ and $j$. $\\text{NMF}_W$ performs NMF updates without updating the values in $W$:\n",
    "$$W_{ij} = [ W_i \\, W_j ]$$\n",
    "$$H_{ij}' = \\text{NMF}_{W_{ij}}(X_{ij})$$\n",
    "$$H_{ij}' = [ H_i' \\, H_j' ]$$\n",
    "$$\\hat{X}_i = W_i H_i$$\n",
    "$$\\hat{X}_j = W_j H_j$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from itertools import islice, permutations, product, chain\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "\n",
    "from magnolia.features.hdf5_iterator import Hdf5Iterator, SplitsIterator\n",
    "from magnolia.features.mixer import FeatureMixer\n",
    "from magnolia.features.wav_iterator import batcher\n",
    "from magnolia.utils.tf_utils import scope_decorator as scope\n",
    "from magnolia.utils.bss_eval import bss_eval_sources\n",
    "from magnolia.factorization.nmf import snmf, nmf_separate\n",
    "from magnolia.utils.postprocessing import reconstruct\n",
    "\n",
    "num_srcs = 2\n",
    "num_steps = 80\n",
    "num_freq_bins = 257\n",
    "num_components = 20\n",
    "sparsity = 0.1\n",
    "num_train_exs = 50\n",
    "num_nmf_iters = 15\n",
    "num_known_spkrs = 30\n",
    "update_weight = 0.05\n",
    "num_test_iters = 80\n",
    "\n",
    "librispeech_dev = \"/local_data/teams/magnolia/librispeech/processed_dev-clean.h5\"\n",
    "# librispeech_train = \"/local_data/teams/magnolia/librispeech/processed_train-clean-100.h5\"\n",
    "# librispeech_test = \"/local_data/teams/magnolia/librispeech/processed_test_clean.h5\"\n",
    "\n",
    "librispeech_dev = \"/Users/patrickc/data/LibriSpeech/processed_dev-clean.h5\"\n",
    "librispeech_train = \"/Users/patrickc/data/LibriSpeech/processed_train-clean-100.h5\"\n",
    "librispeech_test = \"/Users/patrickc/data/LibriSpeech/processed_test_clean.h5\"\n",
    "\n",
    "train_metrics_path = \"/Users/patrickc/src/magnolia/nmf-train-metrics.txt\"\n",
    "inset_results_path = \"/Users/patrickc/src/magnolia/nmf-inset-results.txt\"\n",
    "outset_results_path = \"/Users/patrickc/src/magnolia/nmf-outset-results.txt\"\n",
    "\n",
    "for path in [train_metrics_path,inset_results_path,outset_results_path]:\n",
    "    with open(path,\"w\") as f:\n",
    "        pass\n",
    "\n",
    "def scale_spectrogram(spectrogram):\n",
    "    mag_spec = np.abs(spectrogram)\n",
    "    phases = np.unwrap(np.angle(spectrogram))\n",
    "    \n",
    "    mag_spec = np.sqrt(mag_spec)\n",
    "    M = mag_spec.max()\n",
    "    m = mag_spec.min()\n",
    "    \n",
    "    return (mag_spec - m)/(M - m), phases\n",
    "\n",
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "### Get speaker-specific iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pdb off\n",
    "with open(\"../../data/librispeech/authors/dev-clean-F.txt\") as f:\n",
    "    female_dev = f.read().strip().split('\\n')\n",
    "with open(\"../../data/librispeech/authors/dev-clean-M.txt\") as f:\n",
    "    male_dev = f.read().strip().split('\\n')\n",
    "with open(\"../../data/librispeech/authors/train-clean-100-F.txt\") as f:\n",
    "    female_train = f.read().strip().split('\\n')\n",
    "with open(\"../../data/librispeech/authors/train-clean-100-M.txt\") as f:\n",
    "    male_train = f.read().strip().split('\\n')\n",
    "with open(\"../../data/librispeech/authors/test-clean-F.txt\") as f:\n",
    "    female_test = f.read().strip().split('\\n')\n",
    "with open(\"../../data/librispeech/authors/test-clean-M.txt\") as f:\n",
    "    male_test = f.read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "female_spkrs = [SplitsIterator([0.8, 0.1, 0.,1], hdf5_path=librispeech_train, speaker_keys=[train], shape=(None,)) for train in female_train]\n",
    "female_spkrs_slice = [SplitsIterator([0.8, 0.1, 0.,1], hdf5_path=librispeech_train, speaker_keys=[train], shape=(num_steps,)) for train in female_train]\n",
    "male_spkrs = [SplitsIterator([0.8, 0.1, 0.,1], hdf5_path=librispeech_train, speaker_keys=[train], shape=(None,)) for train in male_train]\n",
    "male_spkrs_slice = [SplitsIterator([0.8, 0.1, 0.,1], hdf5_path=librispeech_train, speaker_keys=[train], shape=(num_steps,)) for train in male_train]\n",
    "\n",
    "female_spkrs_dev = [Hdf5Iterator(hdf5_path=librispeech_dev, speaker_keys=[dev], shape=(None,)) for dev in female_dev]\n",
    "female_spkrs_dev_slice = [Hdf5Iterator(hdf5_path=librispeech_dev, speaker_keys=[dev], shape=(num_steps,)) for dev in female_dev]\n",
    "male_spkrs_dev = [Hdf5Iterator(hdf5_path=librispeech_dev, speaker_keys=[dev], shape=(None,)) for dev in male_dev]\n",
    "male_spkrs_dev_slice = [Hdf5Iterator(hdf5_path=librispeech_dev, speaker_keys=[dev], shape=(num_steps,)) for dev in male_dev]\n",
    "\n",
    "female_spkrs_test = [Hdf5Iterator(hdf5_path=librispeech_test, speaker_keys=[test], shape=(None,)) for test in female_test]\n",
    "female_spkrs_test_slice = [Hdf5Iterator(hdf5_path=librispeech_test, speaker_keys=[test], shape=(num_steps,)) for test in female_test]\n",
    "male_spkrs_test = [Hdf5Iterator(hdf5_path=librispeech_test, speaker_keys=[test], shape=(None,)) for test in male_test]\n",
    "male_spkrs_test_slice = [Hdf5Iterator(hdf5_path=librispeech_test, speaker_keys=[test], shape=(num_steps,)) for test in male_test]\n",
    "\n",
    "female_spkrs_test_slice150 = [Hdf5Iterator(hdf5_path=librispeech_test, speaker_keys=[test], shape=(150,)) for test in female_test]\n",
    "male_spkrs_test_slice150 = [Hdf5Iterator(hdf5_path=librispeech_test, speaker_keys=[test], shape=(150,)) for test in male_test]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Get iterators over dataset splits"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "feature_iterators = [SplitsIterator([0.8,0.1,0.1], librispeech_train, \n",
    "                     shape=(num_steps, None), seed=i) for i in range(num_srcs)]\n",
    "mixed_features = FeatureMixer(feature_iterators, shape=(num_steps, None))\n",
    "data_batches = batcher(mixed_features, batch_size)\n",
    "\n",
    "feature_iterators_val_inset = [SplitsIterator([0.8,0.1,0.1], librispeech_train, \n",
    "                     shape=(num_steps, None), seed=i) for i in range(num_srcs)]\n",
    "for i in range(num_srcs):\n",
    "    feature_iterators_val_inset[i].set_split(1)\n",
    "mixed_features_val_inset = FeatureMixer(feature_iterators_val_inset, shape=(num_steps, None))\n",
    "data_batches_val_inset = batcher(mixed_features_val_inset, batch_size)\n",
    "\n",
    "feature_iterators_test_inset = [SplitsIterator([0.8,0.1,0.1], librispeech_train, \n",
    "                     shape=(num_steps, None), seed=i) for i in range(num_srcs)]\n",
    "for i in range(num_srcs):\n",
    "    feature_iterators_test_inset[i].set_split(2)\n",
    "mixed_features_test_inset = FeatureMixer(feature_iterators_test_inset, shape=(num_steps, None))\n",
    "data_batches_test_inset = batcher(mixed_features_test_inset, batch_size)\n",
    "\n",
    "feature_iterators_val_outset = [Hdf5Iterator(librispeech_dev, \n",
    "                     shape=(num_steps, None), seed=i) for i in range(num_srcs)]\n",
    "mixed_features_val_outset = FeatureMixer(feature_iterators_val_outset, shape=(num_steps, None))\n",
    "data_batches_val_outset = batcher(mixed_features_val_outset, batch_size)\n",
    "\n",
    "feature_iterators_test_outset = [Hdf5Iterator(librispeech_test, \n",
    "                     shape=(num_steps, None), seed=i) for i in range(num_srcs)]\n",
    "mixed_features_test_outset = FeatureMixer(feature_iterators_test_outset, shape=(num_steps, None))\n",
    "data_batches_test_outset = batcher(mixed_features_test_outset, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pdb off\n",
    "spkr_models = []\n",
    "errors = []\n",
    "train_times = []\n",
    "\n",
    "# Set split to train\n",
    "for spkr in chain(female_spkrs, male_spkrs, female_spkrs_slice, male_spkrs_slice):\n",
    "    spkr.set_split(0)\n",
    "\n",
    "TrainRecord = namedtuple('TrainRecord', ['i', 'loss', 'time_delta', 'timestamp', 'batch_size', 'spkr'])\n",
    "for i, spkr in enumerate(chain(female_spkrs_slice[:num_known_spkrs//2], male_spkrs_slice[:num_known_spkrs//2])):\n",
    "    print(\"Speaker\", i)\n",
    "    w = None\n",
    "    h = None\n",
    "    spkr_errors = []\n",
    "   \n",
    "    for j, example in enumerate(islice(spkr,num_train_exs)):\n",
    "        mag, phases = scale_spectrogram(example)\n",
    "        try:\n",
    "            train_start = time.time()\n",
    "            w, h, w_err, h_size, err = snmf(mag.T, num_components, sparsity=sparsity, \n",
    "                                            num_iters=num_nmf_iters, W_init=w, H_init=h, return_errors=True,\n",
    "                                            update_weight=update_weight)\n",
    "            train_end = time.time()\n",
    "        except ValueError as e:\n",
    "            print(\"ValueError encountered\", file=sys.stderr)\n",
    "            print(e, file=sys.stderr)\n",
    "            if \"operands\" not in repr(e):\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "        spkr_errors.extend(err)\n",
    "        # only record final error for each speaker\n",
    "        train_metrics = TrainRecord(\n",
    "            i*num_train_exs + j,\n",
    "            err[-1],\n",
    "            train_end - train_start,\n",
    "            train_start,\n",
    "            1,\n",
    "            i\n",
    "        )\n",
    "        with open(train_metrics_path, \"a\") as f:\n",
    "            print('\\t'.join(map(str,train_metrics)), file=f)\n",
    "    \n",
    "    train_times.append(train_end - train_start)\n",
    "    errors.append(spkr_errors)\n",
    "    plt.figure(figsize=(6,1))\n",
    "    plt.plot(moving_average(errors[-1],30))\n",
    "    plt.show()\n",
    "    spkr_models.append((w,h))\n",
    "\n",
    "    \n",
    "#     plt.figure(figsize=(14,5))\n",
    "#     plt.subplot(1,3,1)\n",
    "#     plt.imshow((w @ h)[:,:100], cmap='bone', origin='lower', aspect=1/4)\n",
    "#     plt.subplot(1,3,2)\n",
    "#     plt.imshow(mag.T[:,:100], cmap='bone', origin='lower', aspect=1/4)\n",
    "#     plt.subplot(1,3,3)\n",
    "#     plt.imshow(w.T, cmap='bone', origin='lower', aspect=6)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "from magnolia.utils.clustering_utils import preprocess_signal\n",
    "test_file = \"/Users/patrickc/Downloads/mixed_signal.wav\"\n",
    "fs, wav = wavfile.read(test_file)\n",
    "wav_spectrogram, x_in = preprocess_signal(wav, fs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference (in-set)\n",
    "\n",
    "Inference retrains just the loadings matrix $H$ in light of a given $W$ and $X$. The resulting reconstructions are qualitatively quite cruddy unless they are used as masks on the original input, in which case the result is about what we expect (0-2 dB improvement)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Inference on seen speakers\n",
    "# Set split to test\n",
    "for spkr in chain(female_spkrs, male_spkrs, female_spkrs_slice, male_spkrs_slice):\n",
    "    spkr.set_split(2)\n",
    "\n",
    "# spkrs_slices = {'all': (female_spkrs_slice[:num_known_spkrs//2] + male_spkrs_slice[num_known_spkrs//2:num_known_spkrs], spkr_models),\n",
    "#                 'mm': (male_spkrs_slice[:num_known_spkrs//2:], spkr_models[num_known_spkrs//2:]),\n",
    "#                 'ff': (female_spkrs_slice[num_known_spkrs//2:], spkr_models[:num_known_spkrs//2])}\n",
    "\n",
    "TestRecord = namedtuple('TestRecord', ['condition', 'loss', 'sdr', 'sir', 'sar', \n",
    "                                       'time_delta', 'timestamp', 'batch_size', 'spkr'])\n",
    "# Sample randomly (MM/MF/FF)\n",
    "\n",
    "spkrs_slices = {'mf': ((female_spkrs_test_slice[:num_known_spkrs//2],\n",
    "                        male_spkrs_test_slice[:num_known_spkrs//2]), \n",
    "                        (spkr_models[:num_known_spkrs//2],\n",
    "                         spkr_models[num_known_spkrs//2:])),\n",
    "                'mm': ((male_spkrs_test_slice[:num_known_spkrs//2],\n",
    "                       male_spkrs_test_slice[:num_known_spkrs//2]), \n",
    "                       (spkr_models[num_known_spkrs//2:],\n",
    "                        spkr_models[num_known_spkrs//2:])),\n",
    "                'ff': ((female_spkrs_test_slice[num_known_spkrs//2:], \n",
    "                        female_spkrs_test_slice[num_known_spkrs//2:]),\n",
    "                      (spkr_models[:num_known_spkrs//2],\n",
    "                       spkr_models[:num_known_spkrs//2])),\n",
    "                'all': ((female_spkrs_test_slice[:num_known_spkrs//2] + male_spkrs_test_slice[:num_known_spkrs//2],\n",
    "                         female_spkrs_test_slice[:num_known_spkrs//2] + male_spkrs_test_slice[:num_known_spkrs//2]), \n",
    "                        (spkr_models,spkr_models))}\n",
    "\n",
    "# for condition, (spkrs_slice, models) in spkrs_slices.items(): \n",
    "for condition, ((spkrs_slice_i, spkrs_slice_j), (models_i, models_j)) in spkrs_slices.items(): \n",
    "    for i in range(num_test_iters):\n",
    "        spkr_i = np.random.randint(len(spkrs_slice_i))\n",
    "        spkr_j = np.random.randint(len(spkrs_slice_j))\n",
    "        if spkr_i == spkr_j:\n",
    "            continue\n",
    "\n",
    "        example_i = next(spkrs_slice_i[spkr_i])\n",
    "        example_j = next(spkrs_slice_i[spkr_j])\n",
    "\n",
    "        mix = example_i + example_j\n",
    "        mix_scl_mag, mix_scl_phs = scale_spectrogram(mix)\n",
    "\n",
    "        # separate\n",
    "        infer_start = time.time()\n",
    "        reco_i, reco_j = nmf_separate(mix_scl_mag.T, [models_i[spkr_i], models_j[spkr_j]], mask=True)\n",
    "        infer_end = time.time()\n",
    "        \n",
    "        # get eval metrics\n",
    "        recon_audio_i = reconstruct(reco_i.T**2, mix, 10000, None, 0.0256)\n",
    "        recon_audio_j = reconstruct(reco_j.T**2, mix, 10000, None, 0.0256)\n",
    "        recon_ref_i = reconstruct(example_i, example_i, 10000, None, 0.0256)\n",
    "        recon_ref_j = reconstruct(example_j, example_j, 10000, None, 0.0256)\n",
    "        recon_mix = reconstruct(mix, mix, 10000, None, 0.0256)\n",
    "        \n",
    "        \n",
    "        base_metrics =  bss_eval_sources(np.stack([recon_ref_i, recon_ref_j]), np.stack([recon_mix, recon_mix]))\n",
    "        predicted_metrics = bss_eval_sources(np.stack([recon_ref_i, recon_ref_j]), np.stack([recon_audio_i, recon_audio_j]))\n",
    "        base_metrics_mean = np.apply_along_axis(np.mean, 1, base_metrics)\n",
    "        predicted_metrics_mean = np.apply_along_axis(np.mean, 1, predicted_metrics)\n",
    "        avg_diff_metrics = [y-x for x, y in zip(base_metrics_mean[:3], predicted_metrics_mean[:3])]\n",
    "        ms_error = np.sum(np.square(reco_j - np.abs(example_j.T))) + np.sum(np.square(reco_i - np.abs(example_i.T)))\n",
    "        test_record = TestRecord(condition, \n",
    "                            ms_error,\n",
    "                            *avg_diff_metrics,\n",
    "                            infer_end-infer_start,\n",
    "                            infer_start,\n",
    "                            1,\n",
    "                            \"{}_{}_{}\".format(condition, spkr_i, spkr_j))\n",
    "        with open(inset_results_path, \"a\") as f:\n",
    "            print('\\t'.join(map(str, test_record)), file=f)\n",
    "#         plt.figure(figsize=(14,5))\n",
    "#         plt.subplot(1,3,1)\n",
    "#         plt.imshow(mix_scl_mag.T, cmap='bone', origin='lower', aspect=1/4)\n",
    "#         plt.subplot(1,3,2)\n",
    "#         plt.imshow(reco_i, cmap='bone', origin='lower', aspect=1/4)\n",
    "#         plt.subplot(1,3,3)\n",
    "#         plt.imshow(reco_j, cmap='bone', origin='lower', aspect=1/4)\n",
    "#         plt.show()\n",
    "\n",
    "#         display.display(display.Audio(reconstruct(reco_i.T**2, mix, 10000, None, 0.0256), rate=10000))\n",
    "#         display.display(display.Audio(reconstruct(reco_j.T**2, mix, 10000, None, 0.0256), rate=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-sample test\n",
    "\n",
    "Above technique only works when you know which set of basis vectors to select for each speaker.  For unseen speakers this is hard. One approach is just to pick the combination of dictionary entries that minimizes the reconstruction error. Unfortunately quadratic in the number of dictionary entries.\n",
    "\n",
    "(Getting the cross-correlation of each basis with the mixture and picking the top two is another idea.)\n",
    "\n",
    "(PCA on the W's also make lots of lots of sense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inference on out of set speakers\n",
    "# spkrs_slices = {'mf': ((female_spkrs_test_slice150[:num_known_spkrs//2],\n",
    "#                         male_spkrs_test_slice150[num_known_spkrs//2:num_known_spkrs]), \n",
    "#                         (spkr_models[:num_known_spkrs//2],\n",
    "#                          spkr_models[num_known_spkrs//2:])),\n",
    "#                 'mm': ((male_spkrs_test_slice150[:num_known_spkrs//2:],\n",
    "#                        male_spkrs_test_slice150[:num_known_spkrs//2:]), \n",
    "#                        (spkr_models[num_known_spkrs//2:],\n",
    "#                         spkr_models[num_known_spkrs//2:])),\n",
    "#                 'ff': ((female_spkrs_test_slice150[num_known_spkrs//2:], \n",
    "#                         female_spkrs_test_slice150[num_known_spkrs//2:]),\n",
    "#                       (spkr_models[:num_known_spkrs//2],\n",
    "#                        spkr_models[:num_known_spkrs//2]))}\n",
    "\n",
    "spkrs_slices = {'mf': ((female_spkrs_test_slice150[:num_known_spkrs//2],\n",
    "                        male_spkrs_test_slice150[:num_known_spkrs//2]), \n",
    "                        (spkr_models[:num_known_spkrs//2],\n",
    "                         spkr_models[num_known_spkrs//2:])),\n",
    "                'mm': ((male_spkrs_test_slice150[:num_known_spkrs//2],\n",
    "                       male_spkrs_test_slice150[:num_known_spkrs//2]), \n",
    "                       (spkr_models[num_known_spkrs//2:],\n",
    "                        spkr_models[num_known_spkrs//2:])),\n",
    "                'ff': ((female_spkrs_test_slice150[num_known_spkrs//2:], \n",
    "                        female_spkrs_test_slice150[num_known_spkrs//2:]),\n",
    "                      (spkr_models[:num_known_spkrs//2],\n",
    "                       spkr_models[:num_known_spkrs//2])),\n",
    "                'all': ((female_spkrs_test_slice150[:num_known_spkrs//2] + male_spkrs_test_slice150[:num_known_spkrs//2],\n",
    "                         female_spkrs_test_slice150[:num_known_spkrs//2] + male_spkrs_test_slice150[:num_known_spkrs//2]), \n",
    "                        (spkr_models,spkr_models))}\n",
    "\n",
    "# for condition, ((spkrs_slice_i, spkrs_slice_j), (models_i, models_j)) in spkrs_slices.items(): \n",
    "num_test_iters = 1\n",
    "for i in range(num_test_iters):\n",
    "    print(\"Test {}\".format(i))\n",
    "#     spkr_i = np.random.randint(len(spkrs_slice_i))\n",
    "#     spkr_j = np.random.randint(len(spkrs_slice_j))\n",
    "#     if spkr_i == spkr_j:\n",
    "#         continue\n",
    "\n",
    "#         example_i = next(spkrs_slice_i[spkr_i])\n",
    "#         example_j = next(spkrs_slice_j[spkr_j])\n",
    "\n",
    "#         mix = example_i + example_j\n",
    "#         mix_scl_mag, mix_scl_phs = scale_spectrogram(mix)\n",
    "\n",
    "    models_i = spkr_models\n",
    "    models_j = spkr_models\n",
    "    mix_scl_mag = x_in\n",
    "    mix = wav_spectrogram\n",
    "    \n",
    "\n",
    "\n",
    "    # loop over choices of speaker model\n",
    "    NmfSearchResult = namedtuple(\"NMFSearchResult\", ['i', 'j', 'error', 'models', 'reconstructions'])\n",
    "    optimal_pair = NmfSearchResult(0, 0, np.inf, [], [])  # i, j, error, reconstructions a and b\n",
    "    search_time = 0\n",
    "    for model_i in range(len(models_i)):\n",
    "        for model_j in range(len(models_j)):\n",
    "            nmf_start = time.time()\n",
    "            reco_i, reco_j = nmf_separate(mix_scl_mag.T, [models_i[model_i], models_j[model_j]], mask=True)\n",
    "            nmf_end = time.time()\n",
    "            search_time += nmf_end - nmf_start\n",
    "            err = np.mean(np.abs(mix_scl_mag.T - (reco_i + reco_j)))\n",
    "\n",
    "            if err < optimal_pair.error:\n",
    "                optimal_pair = NmfSearchResult(model_i, model_j, err, \n",
    "                                               [models_i[model_i][0], models_j[model_j][0]], \n",
    "                                               [reco_i, reco_j])\n",
    "\n",
    "    # Display results\n",
    "    print(\"Used weight matrices {} and {}\".format(optimal_pair.i, optimal_pair.j))\n",
    "\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(mix_scl_mag.T, cmap='bone', origin='lower', aspect=1/4)\n",
    "    plt.subplot(1,3,2)\n",
    "\n",
    "    plt.imshow(optimal_pair.reconstructions[0], cmap='bone', origin='lower', aspect=1/4)\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(optimal_pair.reconstructions[1], cmap='bone', origin='lower', aspect=1/4)\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate\n",
    "    opt_spec_i, opt_spec_j = optimal_pair.reconstructions\n",
    "    mix_audio = reconstruct(mix, mix, 10000, None, 0.0256)\n",
    "#     ref_i = reconstruct(example_i, example_i, 10000, None, 0.0256)\n",
    "#     ref_j = reconstruct(example_j, example_j, 10000, None, 0.0256)\n",
    "    opt_reco_i = reconstruct(optimal_pair.reconstructions[0].T**2, mix, 10000, None, 0.0256)\n",
    "    opt_reco_j = reconstruct(optimal_pair.reconstructions[1].T**2, mix, 10000, None, 0.0256)\n",
    "\n",
    "#     base_metrics =  bss_eval_sources(np.stack([ref_i, ref_j]), np.stack([mix_audio, mix_audio]))\n",
    "#     predicted_metrics = bss_eval_sources(np.stack([ref_i, ref_j]), np.stack([opt_reco_i, opt_reco_j]))\n",
    "#     base_metrics_mean = np.apply_along_axis(np.mean, 1, base_metrics)\n",
    "#     predicted_metrics_mean = np.apply_along_axis(np.mean, 1, predicted_metrics)\n",
    "#     ms_error = (np.sum(np.square(opt_spec_j - np.abs(example_j.T))) + \n",
    "#                np.sum(np.square(opt_spec_i - np.abs(example_i.T))))\n",
    "#     avg_diff_metrics = [y-x for x, y in zip(base_metrics_mean[:3], predicted_metrics_mean[:3])]\n",
    "#     test_record = TestRecord(\n",
    "#         condition,\n",
    "#         ms_error,\n",
    "#         *avg_diff_metrics,\n",
    "#         search_time,\n",
    "#         nmf_end,\n",
    "#         1,\n",
    "#         \"oos_{}_{}_{}\".format(condition, spkr_i, spkr_j)\n",
    "#     )\n",
    "\n",
    "#     with open(outset_results_path, \"a\") as f:\n",
    "#         print('\\t'.join(map(str, test_record)), file=f)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from magnolia.features.data_preprocessing import undo_preemphasis\n",
    "display.display(display.Audio(undo_preemphasis(opt_reco_i.astype(np.float16)/np.abs(opt_reco_i.astype(np.float16).max())),rate=10000))\n",
    "display.display(display.Audio(undo_preemphasis(opt_reco_j.astype(np.float16)/np.abs(opt_reco_j.astype(np.float16).max())),rate=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ins_df = pd.read_csv(inset_results_path, sep='\\t', header=None, names=TestRecord._fields)\n",
    "print(ins_df.groupby('condition').sdr.mean())\n",
    "\n",
    "out_df = pd.read_csv(outset_results_path, sep='\\t', header=None, names=TestRecord._fields)\n",
    "out_df.groupby('condition').sdr.mean()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [magnolia3-cpu]",
   "language": "python",
   "name": "Python [magnolia3-cpu]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
